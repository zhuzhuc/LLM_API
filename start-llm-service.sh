#!/bin/bash

# CPU å¤§æ¨¡å‹æœåŠ¡å¯åŠ¨è„šæœ¬
# ä½œè€…: AI Assistant
# ç”¨é€”: å¯åŠ¨åŸºäº llama.cpp çš„ CPU å¤§æ¨¡å‹æœåŠ¡

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_debug() {
    echo -e "${BLUE}[DEBUG]${NC} $1"
}

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    log_info "æ£€æŸ¥ç³»ç»Ÿä¾èµ–..."
    
    # æ£€æŸ¥ Go
    if ! command -v go &> /dev/null; then
        log_error "Go æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… Go 1.19+"
        exit 1
    fi
    
    # æ£€æŸ¥ llama.cpp æ˜¯å¦ç¼–è¯‘
    if [ ! -f "./llama-cpp/build/bin/llama-server" ]; then
        log_error "llama-server æœªæ‰¾åˆ°ï¼Œè¯·å…ˆç¼–è¯‘ llama.cpp"
        log_info "è¿è¡Œ: cd llama-cpp && mkdir -p build && cd build && cmake .. && make -j\$(nproc)"
        exit 1
    fi
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if [ ! -d "./models" ] || [ -z "$(ls -A ./models/*.gguf 2>/dev/null)" ]; then
        log_error "æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ models ç›®å½•ä¸­æœ‰ .gguf æ ¼å¼çš„æ¨¡å‹æ–‡ä»¶"
        exit 1
    fi
    
    log_info "ä¾èµ–æ£€æŸ¥å®Œæˆ"
}

# åˆ›å»ºç¯å¢ƒé…ç½®
setup_environment() {
    log_info "è®¾ç½®ç¯å¢ƒé…ç½®..."
    
    if [ ! -f ".env" ]; then
        log_info "åˆ›å»º .env æ–‡ä»¶..."
        cp .env.example .env
        log_warn "è¯·æ ¹æ®éœ€è¦ä¿®æ”¹ .env æ–‡ä»¶ä¸­çš„é…ç½®"
    fi
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    mkdir -p logs
    
    log_info "ç¯å¢ƒé…ç½®å®Œæˆ"
}

# å¯åŠ¨åç«¯æœåŠ¡
start_backend() {
    log_info "å¯åŠ¨åç«¯æœåŠ¡..."

    # å®‰è£…ä¾èµ–
    if [ ! -f "backend/go.sum" ]; then
        log_info "åˆå§‹åŒ– Go æ¨¡å—..."
        cd backend && go mod tidy && cd ..
    fi

    # ç¼–è¯‘å¹¶å¯åŠ¨ (ä»é¡¹ç›®æ ¹ç›®å½•å¯åŠ¨ï¼Œè¿™æ ·ç›¸å¯¹è·¯å¾„æ‰æ­£ç¡®)
    log_info "ç¼–è¯‘å¹¶å¯åŠ¨åç«¯æœåŠ¡..."
    cd backend && go build -o llm-server main.go && cd ..
    ./backend/llm-server &
    BACKEND_PID=$!

    log_info "åç«¯æœåŠ¡å·²å¯åŠ¨ (PID: $BACKEND_PID)"
    echo $BACKEND_PID > logs/backend.pid
}

# ç­‰å¾…æœåŠ¡å¯åŠ¨
wait_for_service() {
    log_info "ç­‰å¾…æœåŠ¡å¯åŠ¨..."
    
    for i in {1..30}; do
        if curl -s http://localhost:8080/health > /dev/null 2>&1; then
            log_info "æœåŠ¡å¯åŠ¨æˆåŠŸ!"
            return 0
        fi
        sleep 1
    done
    
    log_error "æœåŠ¡å¯åŠ¨è¶…æ—¶"
    return 1
}

# æ˜¾ç¤ºæœåŠ¡ä¿¡æ¯
show_service_info() {
    log_info "=== CPU å¤§æ¨¡å‹æœåŠ¡å¹³å°ä¿¡æ¯ ==="
    echo
    echo "ğŸš€ æœåŠ¡åœ°å€:"
    echo "   - API æœåŠ¡: http://localhost:8080"
    echo "   - å¥åº·æ£€æŸ¥: http://localhost:8080/health"
    echo "   - ç³»ç»ŸçŠ¶æ€: http://localhost:8080/status"
    echo
    echo "ğŸ“Š æ ¸å¿ƒæ¥å£:"
    echo "   æ¨¡å‹ç®¡ç†:"
    echo "   - GET  /api/v1/models          - è·å–å¯ç”¨æ¨¡å‹"
    echo "   - GET  /api/v1/models/running  - è·å–è¿è¡Œä¸­æ¨¡å‹"
    echo "   - POST /api/v1/models/{name}/start - å¯åŠ¨æ¨¡å‹"
    echo "   - POST /api/v1/models/{name}/chat  - ä¸æ¨¡å‹å¯¹è¯"
    echo
    echo "   OpenAI å…¼å®¹æ¥å£:"
    echo "   - POST /api/v1/v1/chat/completions - èŠå¤©å®Œæˆ"
    echo "   - GET  /api/v1/v1/models           - æ¨¡å‹åˆ—è¡¨"
    echo "   - POST /api/v1/v1/batch            - æ‰¹é‡è¯·æ±‚"
    echo
    echo "   ç›‘æ§å’Œç®¡ç†:"
    echo "   - GET  /api/v1/monitoring/metrics - ç³»ç»ŸæŒ‡æ ‡"
    echo "   - GET  /api/v1/logs               - ç³»ç»Ÿæ—¥å¿—"
    echo "   - GET  /api/v1/discovery/services - æœåŠ¡å‘ç°"
    echo "   - GET  /api/v1/cluster/stats      - é›†ç¾¤çŠ¶æ€"
    echo
    echo "ğŸ“ é‡è¦æ–‡ä»¶:"
    echo "   - é…ç½®æ–‡ä»¶: .env"
    echo "   - æ¨¡å‹é…ç½®: models/model_config.json"
    echo "   - æ—¥å¿—ç›®å½•: logs/"
    echo "   - åç«¯æœåŠ¡: backend/llm-server"
    echo
    echo "ğŸ› ï¸  ç®¡ç†å‘½ä»¤:"
    echo "   - åœæ­¢æœåŠ¡: ./stop-llm-service.sh"
    echo "   - æŸ¥çœ‹æ—¥å¿—: tail -f logs/*.log"
    echo "   - é‡å¯æœåŠ¡: ./restart-llm-service.sh"
    echo "   - é›†ç¾¤ç®¡ç†: curl http://localhost:8080/api/v1/cluster/stats"
    echo
}

# åˆ›å»ºåœæ­¢è„šæœ¬
create_stop_script() {
    cat > stop-llm-service.sh << 'EOF'
#!/bin/bash

# åœæ­¢ LLM æœåŠ¡è„šæœ¬

RED='\033[0;31m'
GREEN='\033[0;32m'
NC='\033[0m'

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# åœæ­¢åç«¯æœåŠ¡
if [ -f "logs/backend.pid" ]; then
    BACKEND_PID=$(cat logs/backend.pid)
    if kill -0 $BACKEND_PID 2>/dev/null; then
        log_info "åœæ­¢åç«¯æœåŠ¡ (PID: $BACKEND_PID)..."
        kill $BACKEND_PID
        rm logs/backend.pid
    else
        log_error "åç«¯æœåŠ¡è¿›ç¨‹ä¸å­˜åœ¨"
    fi
else
    log_error "æœªæ‰¾åˆ°åç«¯æœåŠ¡ PID æ–‡ä»¶"
fi

# åœæ­¢æ‰€æœ‰ llama-server è¿›ç¨‹
log_info "åœæ­¢æ‰€æœ‰æ¨¡å‹æœåŠ¡..."
pkill -f "llama-server" || true

log_info "æœåŠ¡å·²åœæ­¢"
EOF

    chmod +x stop-llm-service.sh
}

# ä¸»å‡½æ•°
main() {
    log_info "å¯åŠ¨ CPU å¤§æ¨¡å‹æœåŠ¡å¹³å°..."
    
    check_dependencies
    setup_environment
    create_stop_script
    start_backend
    
    if wait_for_service; then
        show_service_info
        
        log_info "æœåŠ¡å¯åŠ¨å®Œæˆ! æŒ‰ Ctrl+C åœæ­¢æœåŠ¡"
        
        # ç­‰å¾…ä¸­æ–­ä¿¡å·
        trap 'log_info "æ­£åœ¨åœæ­¢æœåŠ¡..."; ./stop-llm-service.sh; exit 0' INT
        
        # ä¿æŒè„šæœ¬è¿è¡Œ
        while true; do
            sleep 1
        done
    else
        log_error "æœåŠ¡å¯åŠ¨å¤±è´¥"
        ./stop-llm-service.sh
        exit 1
    fi
}

# è¿è¡Œä¸»å‡½æ•°
main "$@"
